{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO270+ItXISWLVV3sNQ5ePW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunchocopie/oss25/blob/main/5_15_pytest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8MyfH0TdGtO",
        "outputId": "9ed2cdc6-b429-475a-9356-788797180f6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (8.3.5)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest) (1.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytest # pytest 설치"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYVr9L5afEs1",
        "outputId": "c8e41c11-f050-4820-aa2d-674a01d0714a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: pytest [options] [file_or_dir] [file_or_dir] [...]\n",
            "\n",
            "positional arguments:\n",
            "  file_or_dir\n",
            "\n",
            "general:\n",
            "  -k EXPRESSION         Only run tests which match the given substring\n",
            "                        expression. An expression is a Python evaluable\n",
            "                        expression where all names are substring-matched against\n",
            "                        test names and their parent classes. Example: -k\n",
            "                        'test_method or test_other' matches all test functions\n",
            "                        and classes whose name contains 'test_method' or\n",
            "                        'test_other', while -k 'not test_method' matches those\n",
            "                        that don't contain 'test_method' in their names. -k 'not\n",
            "                        test_method and not test_other' will eliminate the\n",
            "                        matches. Additionally keywords are matched to classes\n",
            "                        and functions containing extra names in their\n",
            "                        'extra_keyword_matches' set, as well as functions which\n",
            "                        have names assigned directly to them. The matching is\n",
            "                        case-insensitive.\n",
            "  -m MARKEXPR           Only run tests matching given mark expression. For\n",
            "                        example: -m 'mark1 and not mark2'.\n",
            "  --markers             show markers (builtin, plugin and per-project ones).\n",
            "  -x, --exitfirst       Exit instantly on first error or failed test\n",
            "  --fixtures, --funcargs\n",
            "                        Show available fixtures, sorted by plugin appearance\n",
            "                        (fixtures with leading '_' are only shown with '-v')\n",
            "  --fixtures-per-test   Show fixtures per test\n",
            "  --pdb                 Start the interactive Python debugger on errors or\n",
            "                        KeyboardInterrupt\n",
            "  --pdbcls=modulename:classname\n",
            "                        Specify a custom interactive Python debugger for use\n",
            "                        with --pdb.For example:\n",
            "                        --pdbcls=IPython.terminal.debugger:TerminalPdb\n",
            "  --trace               Immediately break when running each test\n",
            "  --capture=method      Per-test capturing method: one of fd|sys|no|tee-sys\n",
            "  -s                    Shortcut for --capture=no\n",
            "  --runxfail            Report the results of xfail tests as if they were not\n",
            "                        marked\n",
            "  --lf, --last-failed   Rerun only the tests that failed at the last run (or all\n",
            "                        if none failed)\n",
            "  --ff, --failed-first  Run all tests, but run the last failures first. This may\n",
            "                        re-order tests and thus lead to repeated fixture\n",
            "                        setup/teardown.\n",
            "  --nf, --new-first     Run tests from new files first, then the rest of the\n",
            "                        tests sorted by file mtime\n",
            "  --cache-show=[CACHESHOW]\n",
            "                        Show cache contents, don't perform collection or tests.\n",
            "                        Optional argument: glob (default: '*').\n",
            "  --cache-clear         Remove all cache contents at start of test run\n",
            "  --lfnf={all,none}, --last-failed-no-failures={all,none}\n",
            "                        With ``--lf``, determines whether to execute tests when\n",
            "                        there are no previously (known) failures or when no\n",
            "                        cached ``lastfailed`` data was found. ``all`` (the\n",
            "                        default) runs the full test suite again. ``none`` just\n",
            "                        emits a message about no known failures and exits\n",
            "                        successfully.\n",
            "  --sw, --stepwise      Exit on test failure and continue from last failing test\n",
            "                        next time\n",
            "  --sw-skip, --stepwise-skip\n",
            "                        Ignore the first failing test but stop on the next\n",
            "                        failing test. Implicitly enables --stepwise.\n",
            "\n",
            "Reporting:\n",
            "  --durations=N         Show N slowest setup/test durations (N=0 for all)\n",
            "  --durations-min=N     Minimal duration in seconds for inclusion in slowest\n",
            "                        list. Default: 0.005.\n",
            "  -v, --verbose         Increase verbosity\n",
            "  --no-header           Disable header\n",
            "  --no-summary          Disable summary\n",
            "  --no-fold-skipped     Do not fold skipped tests in short summary.\n",
            "  -q, --quiet           Decrease verbosity\n",
            "  --verbosity=VERBOSE   Set verbosity. Default: 0.\n",
            "  -r chars              Show extra test summary info as specified by chars:\n",
            "                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,\n",
            "                        (p)assed, (P)assed with output, (a)ll except passed\n",
            "                        (p/P), or (A)ll. (w)arnings are enabled by default (see\n",
            "                        --disable-warnings), 'N' can be used to reset the list.\n",
            "                        (default: 'fE').\n",
            "  --disable-warnings, --disable-pytest-warnings\n",
            "                        Disable warnings summary\n",
            "  -l, --showlocals      Show locals in tracebacks (disabled by default)\n",
            "  --no-showlocals       Hide locals in tracebacks (negate --showlocals passed\n",
            "                        through addopts)\n",
            "  --tb=style            Traceback print mode (auto/long/short/line/native/no)\n",
            "  --xfail-tb            Show tracebacks for xfail (as long as --tb != no)\n",
            "  --show-capture={no,stdout,stderr,log,all}\n",
            "                        Controls how captured stdout/stderr/log is shown on\n",
            "                        failed tests. Default: all.\n",
            "  --full-trace          Don't cut any tracebacks (default is to cut)\n",
            "  --color=color         Color terminal output (yes/no/auto)\n",
            "  --code-highlight={yes,no}\n",
            "                        Whether code should be highlighted (only if --color is\n",
            "                        also enabled). Default: yes.\n",
            "  --pastebin=mode       Send failed|all info to bpaste.net pastebin service\n",
            "  --junit-xml=path      Create junit-xml style report file at given path\n",
            "  --junit-prefix=str    Prepend prefix to classnames in junit-xml output\n",
            "\n",
            "pytest-warnings:\n",
            "  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS\n",
            "                        Set which warnings to report, see -W option of Python\n",
            "                        itself\n",
            "  --maxfail=num         Exit after first num failures or errors\n",
            "  --strict-config       Any warnings encountered while parsing the `pytest`\n",
            "                        section of the configuration file raise errors\n",
            "  --strict-markers      Markers not registered in the `markers` section of the\n",
            "                        configuration file raise errors\n",
            "  --strict              (Deprecated) alias to --strict-markers\n",
            "  -c FILE, --config-file=FILE\n",
            "                        Load configuration from `FILE` instead of trying to\n",
            "                        locate one of the implicit configuration files.\n",
            "  --continue-on-collection-errors\n",
            "                        Force test execution even if collection errors occur\n",
            "  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:\n",
            "                        'root_dir', './root_dir', 'root_dir/another_dir/';\n",
            "                        absolute path: '/home/user/root_dir'; path with\n",
            "                        variables: '$HOME/root_dir'.\n",
            "\n",
            "collection:\n",
            "  --collect-only, --co  Only collect tests, don't execute them\n",
            "  --pyargs              Try to interpret all arguments as Python packages\n",
            "  --ignore=path         Ignore path during collection (multi-allowed)\n",
            "  --ignore-glob=path    Ignore path pattern during collection (multi-allowed)\n",
            "  --deselect=nodeid_prefix\n",
            "                        Deselect item (via node id prefix) during collection\n",
            "                        (multi-allowed)\n",
            "  --confcutdir=dir      Only load conftest.py's relative to specified dir\n",
            "  --noconftest          Don't load any conftest.py files\n",
            "  --keep-duplicates     Keep duplicate tests\n",
            "  --collect-in-virtualenv\n",
            "                        Don't ignore tests in a local virtualenv directory\n",
            "  --import-mode={prepend,append,importlib}\n",
            "                        Prepend/append to sys.path when importing test modules\n",
            "                        and conftest files. Default: prepend.\n",
            "  --doctest-modules     Run doctests in all .py modules\n",
            "  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}\n",
            "                        Choose another output format for diffs on doctest\n",
            "                        failure\n",
            "  --doctest-glob=pat    Doctests file matching pattern, default: test*.txt\n",
            "  --doctest-ignore-import-errors\n",
            "                        Ignore doctest collection errors\n",
            "  --doctest-continue-on-failure\n",
            "                        For a given doctest, continue to run after the first\n",
            "                        failure\n",
            "\n",
            "test session debugging and configuration:\n",
            "  --basetemp=dir        Base temporary directory for this test run. (Warning:\n",
            "                        this directory is removed if it exists.)\n",
            "  -V, --version         Display pytest version and information about plugins.\n",
            "                        When given twice, also display information about\n",
            "                        plugins.\n",
            "  -h, --help            Show help message and configuration info\n",
            "  -p name               Early-load given plugin module name or entry point\n",
            "                        (multi-allowed). To avoid loading of plugins, use the\n",
            "                        `no:` prefix, e.g. `no:doctest`.\n",
            "  --trace-config        Trace considerations of conftest.py files\n",
            "  --debug=[DEBUG_FILE_NAME]\n",
            "                        Store internal tracing debug information in this log\n",
            "                        file. This file is opened with 'w' and truncated as a\n",
            "                        result, care advised. Default: pytestdebug.log.\n",
            "  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI\n",
            "                        Override ini option with \"option=value\" style, e.g. `-o\n",
            "                        xfail_strict=True -o cache_dir=cache`.\n",
            "  --assert=MODE         Control assertion debugging tools.\n",
            "                        'plain' performs no assertion debugging.\n",
            "                        'rewrite' (the default) rewrites assert statements in\n",
            "                        test modules on import to provide assert expression\n",
            "                        information.\n",
            "  --setup-only          Only setup fixtures, do not execute tests\n",
            "  --setup-show          Show setup of fixtures while executing tests\n",
            "  --setup-plan          Show what fixtures and tests would be executed but don't\n",
            "                        execute anything\n",
            "\n",
            "logging:\n",
            "  --log-level=LEVEL     Level of messages to catch/display. Not set by default,\n",
            "                        so it depends on the root/parent log handler's effective\n",
            "                        level, where it is \"WARNING\" by default.\n",
            "  --log-format=LOG_FORMAT\n",
            "                        Log format used by the logging module\n",
            "  --log-date-format=LOG_DATE_FORMAT\n",
            "                        Log date format used by the logging module\n",
            "  --log-cli-level=LOG_CLI_LEVEL\n",
            "                        CLI logging level\n",
            "  --log-cli-format=LOG_CLI_FORMAT\n",
            "                        Log format used by the logging module\n",
            "  --log-cli-date-format=LOG_CLI_DATE_FORMAT\n",
            "                        Log date format used by the logging module\n",
            "  --log-file=LOG_FILE   Path to a file when logging will be written to\n",
            "  --log-file-mode={w,a}\n",
            "                        Log file open mode\n",
            "  --log-file-level=LOG_FILE_LEVEL\n",
            "                        Log file logging level\n",
            "  --log-file-format=LOG_FILE_FORMAT\n",
            "                        Log format used by the logging module\n",
            "  --log-file-date-format=LOG_FILE_DATE_FORMAT\n",
            "                        Log date format used by the logging module\n",
            "  --log-auto-indent=LOG_AUTO_INDENT\n",
            "                        Auto-indent multiline messages passed to the logging\n",
            "                        module. Accepts true|on, false|off or an integer.\n",
            "  --log-disable=LOGGER_DISABLE\n",
            "                        Disable a logger by name. Can be passed multiple times.\n",
            "\n",
            "LangSmith:\n",
            "  --langsmith-output    Use LangSmith output (requires 'rich').\n",
            "\n",
            "typeguard:\n",
            "  --typeguard-packages=TYPEGUARD_PACKAGES\n",
            "                        comma separated name list of packages and modules to\n",
            "                        instrument for type checking, or :all: to instrument all\n",
            "                        modules loaded after typeguard\n",
            "  --typeguard-debug-instrumentation\n",
            "                        print all instrumented code to stderr\n",
            "  --typeguard-typecheck-fail-callback=TYPEGUARD_TYPECHECK_FAIL_CALLBACK\n",
            "                        a module:varname (e.g. typeguard:warn_on_error)\n",
            "                        reference to a function that is called (with the\n",
            "                        exception, and memo object as arguments) to handle a\n",
            "                        TypeCheckError\n",
            "  --typeguard-forward-ref-policy={ERROR,WARN,IGNORE}\n",
            "                        determines how to deal with unresolveable forward\n",
            "                        references in type annotations\n",
            "  --typeguard-collection-check-strategy={FIRST_ITEM,ALL_ITEMS}\n",
            "                        determines how thoroughly to check collections (list,\n",
            "                        dict, etc)\n",
            "\n",
            "[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg|pyproject.toml file found:\n",
            "\n",
            "  markers (linelist):   Register new markers for test functions\n",
            "  empty_parameter_set_mark (string):\n",
            "                        Default marker for empty parametersets\n",
            "  norecursedirs (args): Directory patterns to avoid for recursion\n",
            "  testpaths (args):     Directories to search for tests when no files or\n",
            "                        directories are given on the command line\n",
            "  filterwarnings (linelist):\n",
            "                        Each line specifies a pattern for\n",
            "                        warnings.filterwarnings. Processed after\n",
            "                        -W/--pythonwarnings.\n",
            "  consider_namespace_packages (bool):\n",
            "                        Consider namespace packages when resolving module names\n",
            "                        during import\n",
            "  usefixtures (args):   List of default fixtures to be used with this project\n",
            "  python_files (args):  Glob-style file patterns for Python test module\n",
            "                        discovery\n",
            "  python_classes (args):\n",
            "                        Prefixes or glob names for Python test class discovery\n",
            "  python_functions (args):\n",
            "                        Prefixes or glob names for Python test function and\n",
            "                        method discovery\n",
            "  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):\n",
            "                        Disable string escape non-ASCII characters, might cause\n",
            "                        unwanted side effects(use at your own risk)\n",
            "  console_output_style (string):\n",
            "                        Console output: \"classic\", or with additional progress\n",
            "                        information (\"progress\" (percentage) | \"count\" |\n",
            "                        \"progress-even-when-capture-no\" (forces progress even\n",
            "                        when capture=no)\n",
            "  verbosity_test_cases (string):\n",
            "                        Specify a verbosity level for test case execution,\n",
            "                        overriding the main level. Higher levels will provide\n",
            "                        more detailed information about each test case executed.\n",
            "  xfail_strict (bool):  Default for the strict parameter of xfail markers when\n",
            "                        not given explicitly (default: False)\n",
            "  tmp_path_retention_count (string):\n",
            "                        How many sessions should we keep the `tmp_path`\n",
            "                        directories, according to `tmp_path_retention_policy`.\n",
            "  tmp_path_retention_policy (string):\n",
            "                        Controls which directories created by the `tmp_path`\n",
            "                        fixture are kept around, based on test outcome.\n",
            "                        (all/failed/none)\n",
            "  enable_assertion_pass_hook (bool):\n",
            "                        Enables the pytest_assertion_pass hook. Make sure to\n",
            "                        delete any previously generated pyc cache files.\n",
            "  verbosity_assertions (string):\n",
            "                        Specify a verbosity level for assertions, overriding the\n",
            "                        main level. Higher levels will provide more detailed\n",
            "                        explanation when an assertion fails.\n",
            "  junit_suite_name (string):\n",
            "                        Test suite name for JUnit report\n",
            "  junit_logging (string):\n",
            "                        Write captured log messages to JUnit report: one of\n",
            "                        no|log|system-out|system-err|out-err|all\n",
            "  junit_log_passing_tests (bool):\n",
            "                        Capture log information for passing tests to JUnit\n",
            "                        report:\n",
            "  junit_duration_report (string):\n",
            "                        Duration time to report: one of total|call\n",
            "  junit_family (string):\n",
            "                        Emit XML for schema: one of legacy|xunit1|xunit2\n",
            "  doctest_optionflags (args):\n",
            "                        Option flags for doctests\n",
            "  doctest_encoding (string):\n",
            "                        Encoding used for doctest files\n",
            "  cache_dir (string):   Cache directory path\n",
            "  log_level (string):   Default value for --log-level\n",
            "  log_format (string):  Default value for --log-format\n",
            "  log_date_format (string):\n",
            "                        Default value for --log-date-format\n",
            "  log_cli (bool):       Enable log display during test run (also known as \"live\n",
            "                        logging\")\n",
            "  log_cli_level (string):\n",
            "                        Default value for --log-cli-level\n",
            "  log_cli_format (string):\n",
            "                        Default value for --log-cli-format\n",
            "  log_cli_date_format (string):\n",
            "                        Default value for --log-cli-date-format\n",
            "  log_file (string):    Default value for --log-file\n",
            "  log_file_mode (string):\n",
            "                        Default value for --log-file-mode\n",
            "  log_file_level (string):\n",
            "                        Default value for --log-file-level\n",
            "  log_file_format (string):\n",
            "                        Default value for --log-file-format\n",
            "  log_file_date_format (string):\n",
            "                        Default value for --log-file-date-format\n",
            "  log_auto_indent (string):\n",
            "                        Default value for --log-auto-indent\n",
            "  pythonpath (paths):   Add paths to sys.path\n",
            "  faulthandler_timeout (string):\n",
            "                        Dump the traceback of all threads if a test takes more\n",
            "                        than TIMEOUT seconds to finish\n",
            "  addopts (args):       Extra command line options\n",
            "  minversion (string):  Minimally required pytest version\n",
            "  required_plugins (args):\n",
            "                        Plugins that must be present for pytest to run\n",
            "  typeguard-packages (linelist):\n",
            "                        comma separated name list of packages and modules to\n",
            "                        instrument for type checking, or :all: to instrument all\n",
            "                        modules loaded after typeguard\n",
            "  typeguard-debug-instrumentation (bool):\n",
            "                        print all instrumented code to stderr\n",
            "  typeguard-typecheck-fail-callback (string):\n",
            "                        a module:varname (e.g. typeguard:warn_on_error)\n",
            "                        reference to a function that is called (with the\n",
            "                        exception, and memo object as arguments) to handle a\n",
            "                        TypeCheckError\n",
            "  typeguard-forward-ref-policy (string):\n",
            "                        determines how to deal with unresolveable forward\n",
            "                        references in type annotations\n",
            "  typeguard-collection-check-strategy (string):\n",
            "                        determines how thoroughly to check collections (list,\n",
            "                        dict, etc)\n",
            "\n",
            "Environment variables:\n",
            "  CI                       When set (regardless of value), pytest knows it is running in a CI process and does not truncate summary info\n",
            "  BUILD_NUMBER             Equivalent to CI\n",
            "  PYTEST_ADDOPTS           Extra command line options\n",
            "  PYTEST_PLUGINS           Comma-separated plugins to load during startup\n",
            "  PYTEST_DISABLE_PLUGIN_AUTOLOAD Set to disable plugin auto-loading\n",
            "  PYTEST_DEBUG             Set to enable debug tracing of pytest's internals\n",
            "\n",
            "\n",
            "to see available markers type: pytest --markers\n",
            "to see available fixtures type: pytest --fixtures\n",
            "(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_sample.py\n",
        "import pytest\n",
        "\n",
        "@pytest.mark.set1\n",
        "def test_file1_method1():\n",
        "  x=5\n",
        "  y=6\n",
        "  assert x + 1 == y, \"test failed\"\n",
        "  assert x == y, \"test failed\"\n",
        "\n",
        "@pytest.mark.set2\n",
        "def test_file1_method2():\n",
        "  x=5\n",
        "  y=6\n",
        "  assert x + 1 == y, \"test failed\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwVZiIZbfTxD",
        "outputId": "f87c8998-77bc-457d-a466-dbc2f882ffa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test_sample.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z40UqAAvgGXb",
        "outputId": "dd7a49f6-204d-411e-fba7-2e45af3525a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.11.12, pytest-8.3.5, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.3.42, anyio-4.9.0, typeguard-4.4.2\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                              \u001b[0m\n",
            "\n",
            "test_sample.py \u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                        [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________________________ test_file1_method1 ______________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_file1_method1\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "      x=\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "      y=\u001b[94m6\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "      \u001b[94massert\u001b[39;49;00m x + \u001b[94m1\u001b[39;49;00m == y, \u001b[33m\"\u001b[39;49;00m\u001b[33mtest failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">     \u001b[94massert\u001b[39;49;00m x == y, \u001b[33m\"\u001b[39;49;00m\u001b[33mtest failed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE     AssertionError: test failed\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 5 == 6\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_sample.py\u001b[0m:7: AssertionError\n",
            "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m test_sample.py::\u001b[1mtest_file1_method1\u001b[0m - AssertionError: test failed\n",
            "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test -m set2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGsdmhOei3nR",
        "outputId": "7520c195-5d13-4e82-d767-f2f11af3559f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.11.12, pytest-8.3.5, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.3.42, anyio-4.9.0, typeguard-4.4.2\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items / 1 deselected / 1 selected                                  \u001b[0m\n",
            "\n",
            "test_sample.py \u001b[32m.\u001b[0m\u001b[33m                                                         [100%]\u001b[0m\n",
            "\n",
            "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
            "test_sample.py:3\n",
            "  /content/test_sample.py:3: PytestUnknownMarkWarning: Unknown pytest.mark.set1 - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
            "    @pytest.mark.set1\n",
            "\n",
            "test_sample.py:10\n",
            "  /content/test_sample.py:10: PytestUnknownMarkWarning: Unknown pytest.mark.set2 - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
            "    @pytest.mark.set2\n",
            "\n",
            "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
            "\u001b[33m================= \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m1 deselected\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m ==================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_fixture.py\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture\n",
        "def supply_AA_BB_CC():\n",
        "  aa = 25\n",
        "  bb = 35\n",
        "  cc = 45\n",
        "  return [aa, bb, cc]\n",
        "\n",
        "def test_comparewithAA(supply_AA_BB_CC):\n",
        "  zz = 35\n",
        "  assert supply_AA_BB_CC[0] == z, \"aa and zz comparison failed\"\n",
        "\n",
        "def my_func(x):\n",
        "  return x**x\n",
        "\n",
        "def print_user(name):\n",
        "  print(name)\n",
        "\n",
        "def composite_func(func, x, name):\n",
        "  print_user(name)\n",
        "  func(x)\n",
        "\n",
        "#composite_func(my_func, x, name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMtxI6O9je0c",
        "outputId": "c26d59e8-cf3a-42ea-d548-d0de080c22ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_fixture.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "\n",
        "@pytest.mark.parametrize(\"input1, input2, output\", [(5,5,10), (3,5,12)])\n",
        "\n",
        "def test_add(input1, input2, output):\n",
        "  assert input1+input2 == output, \"failed\""
      ],
      "metadata": {
        "id": "Sxv1hFlbntAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test -m test_add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8l8S2tGoeZu",
        "outputId": "e91930d0-4706-43a6-f52e-4e73cef1f2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.11.12, pytest-8.3.5, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.3.42, anyio-4.9.0, typeguard-4.4.2\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items / 2 deselected / 0 selected                                  \u001b[0m\n",
            "\n",
            "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
            "test_sample.py:3\n",
            "  /content/test_sample.py:3: PytestUnknownMarkWarning: Unknown pytest.mark.set1 - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
            "    @pytest.mark.set1\n",
            "\n",
            "test_sample.py:10\n",
            "  /content/test_sample.py:10: PytestUnknownMarkWarning: Unknown pytest.mark.set2 - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
            "    @pytest.mark.set2\n",
            "\n",
            "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
            "\u001b[33m====================== \u001b[33m\u001b[1m2 deselected\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m =======================\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}